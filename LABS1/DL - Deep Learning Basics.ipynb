{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Basics\n",
    "\n",
    "**Objectives**\n",
    "* Use TensorFlow to create simple network architectures\n",
    "* Understand the basics of creating a Deep Learning solution to a problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will use the \"[OCR dataset](http://ai.stanford.edu/~btaskar/ocr/)\" collected by the MIT Spoken Language System Groups, which contains handwritten letters. For ease of use, the data has already been put into Numpy arrays and can be loaded directly into the Notebook or any Python script from the \"letters_X.npy\" and \"letters_Y.npy\" files.\n",
    "\n",
    "The dataset contains 52152 letters. The letters_X.npy file contains the 16x8 pixels images (flattened to a 128-size vector), and the letters_Y.npy file contains the supervision, with for each letter a 26-size binary vector. The letter \"A\" would be represented by a \"1\" at position 0 in the vector ([1 0 0 0 ... 0]), whereas the letter \"Z\" would be represented by a \"1\" at position 25 ([0 0 0 ... 0 1]). The following code loads the 64 first letters in the dataset and displays the images and supervision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAJOCAYAAABP8PaaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bfb729ba20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load dataset\n",
    "letters_X = np.load('letters_X.npy')\n",
    "letters_Y = np.load('letters_Y.npy')\n",
    "imsize = (16,8)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(64):\n",
    "    plt.subplot(8,8,i+1)\n",
    "    plt.imshow(letters_X[i].reshape(imsize), cmap=plt.cm.gray)\n",
    "    plt.title(chr(97+letters_Y[i].argmax()))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training and test sets\n",
    "\n",
    "Split this dataset into a **training set**, a **validation set** and a **test set**. \n",
    "\n",
    "How can you make sure that they all share the same prior probability distribution ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## -- Your code here -- ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Deep Neural Network with TensorFlow : a fake example\n",
    "\n",
    "[TensorFlow](https://www.tensorflow.org/) is an \"open source machine learning framework\". It contains many operations which allow us to build, train and run Deep Neural Networks and to take full advantage of GPU acceleration.\n",
    "\n",
    "The process of creating a neural network with TensorFlow contains different steps :\n",
    "\n",
    "1. Creating the \"Network Graph\"\n",
    "2. Defining the cost function and the optimizer\n",
    "3. Training the network on the training set\n",
    "4. Evaluating the network on the test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the \"Network Graph\"\n",
    "\n",
    "The Network Graph contains all the operations that will be used for learning and in production. The network is built by \"layers\", connecting the output of an operation to the input of the next one. Let's use a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=int32) Tensor(\"Const_1:0\", shape=(), dtype=int32) Tensor(\"Add:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Creating two \"inputs\"\n",
    "a = tf.constant(1)\n",
    "b = tf.constant(2)\n",
    "\n",
    "# Creating an operation\n",
    "c = tf.add(a, b)\n",
    "\n",
    "print(a,b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important thing to note is that, when building the graph, none of those operations are actually evaluated. We have just defined them. To execute graph operations, we have to define a session, and to evaluate the operation within the session :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "c_evaluated = c.eval(session=sess)\n",
    "print(c_evaluated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting type of element in the graph is the \"placeholder\". A placeholder can be defined as an object of a certain type and size, whose value will be given only during the evaluation. Let's reset the graph and recreate it with placeholders instead of the constant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(), dtype=float32) Tensor(\"Placeholder_1:0\", shape=(), dtype=float32) Tensor(\"Add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # If we don't do that, we will keep adding new operations to the graph !\n",
    "\n",
    "# Creating placeholder\n",
    "a = tf.placeholder(tf.float32, shape=()) # Use shape = () if the object is a scalar\n",
    "b = tf.placeholder(tf.float32, shape=())\n",
    "\n",
    "# Creating opeartion\n",
    "c = tf.add(a,b)\n",
    "\n",
    "print(a,b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to simply evaluate `c`, we will get an error: \n",
    "> \"You must feed a value for placeholder tensor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder' with dtype float\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Add/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_7_Add\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder', defined at:\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-310793b4603c>\", line 4, in <module>\n    a = tf.placeholder(tf.float32, shape=()) # Use shape = () if the object is a scalar\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1599, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3090, in _placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Add/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_7_Add\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype float\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Add/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_7_Add\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d464cb7cbf18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m     \"\"\"\n\u001b[1;32m--> 570\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_dup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   4453\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4454\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 4455\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype float\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Add/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_7_Add\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder', defined at:\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-310793b4603c>\", line 4, in <module>\n    a = tf.placeholder(tf.float32, shape=()) # Use shape = () if the object is a scalar\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1599, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3090, in _placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Administrateur\\Anaconda2\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Add/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_7_Add\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "print(c.eval(session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do that using the `feed_dict` argument of `eval`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n"
     ]
    }
   ],
   "source": [
    "c_evaluated = c.eval(session=sess, feed_dict={a: 5, b: 3})\n",
    "print(c_evaluated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start building something a bit more complex. The following example creates a network with the following characteristics :\n",
    "\n",
    "* As **input**, a batch of 10x10x1 pixels images.\n",
    "* One \"**convolutional layer**\" with 16 3x3 kernel and the \"Leaky ReLU\" activation function\n",
    "* One operation to **flatten** the resulting \"feature maps\" into vectors\n",
    "* One \"**dense layer**\" with as output a size 5 vector and the \"Leaky ReLU\" activation function\n",
    "* One \"**softmax**\" layer applied at the end to get a \"probability distribution\" for the 5 \"classes\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8, 8, 16)\n",
      "(?, 1024)\n",
      "(?, 5)\n",
      "(?, 5)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 10, 10, 1]) # \"None\" is interpreted in this case by TensorFlow as \"unknown\"\n",
    "conv = tf.layers.conv2d(X, 16, 3, activation=tf.nn.leaky_relu, kernel_initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "print(conv.get_shape()) # What's the effect of the convolution on the shape of the output ? Why ?\n",
    "flat = tf.layers.flatten(conv)\n",
    "print(flat.get_shape())\n",
    "dense = tf.layers.dense(flat, 5, activation=tf.nn.leaky_relu)\n",
    "print(dense.get_shape())\n",
    "softmax = tf.nn.softmax(dense)\n",
    "print(softmax.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to evaluate the \"ouput\" of our network, we must give as input a batch of 10x10x1 images. Let's create 2 random images, and get the output :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAC7CAYAAABFJnSnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c23b4579b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17761146  0.20413478  0.14053269  0.20804606  0.26967505]\n",
      " [ 0.17995864  0.14955208  0.15240826  0.26249883  0.25558224]]\n"
     ]
    }
   ],
   "source": [
    "Xrand = np.random.random((2,10,10,1))\n",
    "# Show the two images:\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(Xrand[0,:,:,0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(Xrand[1,:,:,0])\n",
    "plt.show()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "output = softmax.eval(session=sess, feed_dict={X: Xrand})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the cost function and the optimizer\n",
    "\n",
    "As we can see, for each of the two \"images\", we have a size-5 \"output\" vector. This is of course at the moment all meaningless, unless we train this network to actually *do* something.\n",
    "\n",
    "In a supervised problem, we have *two* inputs: the images, and the target output. We also have a cost function which compares the \"target\" and the \"prediction\". In classification problems, the most common cost function is the cross-entropy. In regression problems, it's the mean square error. Let's create the target and the cross-entropy cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?,)\n"
     ]
    }
   ],
   "source": [
    "target = tf.placeholder(tf.float32, [None, 5]) # Target must have the same shape as the output\n",
    "cost = tf.nn.softmax_cross_entropy_with_logits(labels=target, logits=dense) \n",
    "# Note that the \"softmax cross entropy\" function already does the \"softmax\" operation, so we don't do logits=softmax but logits=dense.\n",
    "print(cost.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this computes a cost \"per image\". When training a neural network, we want to have a cost \"per batch\", which will usually be simply the mean value of the \"per image\" cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "cost_avg = tf.reduce_mean(cost)\n",
    "print(cost_avg.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to define the optimizer. Tensorflow provides different options, which are all variants of the simple Gradient Descent algorithm. We will use here the \"Adam\" optimizer, which tend to perform well. With this optimizer, we can define the \"training step\" of the algorithm, meaning \"what the training algorithm must do at each iteration\". In this case, we want to use the optimizer to minimize the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "trainingStep = optimizer.minimize(cost_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network on the training set\n",
    "\n",
    "Right now, we don't have a training set for this fake network. Let's quickly generate one, with some simple rules:\n",
    "\n",
    "* We will generate 1000 images\n",
    "* Each image will be assigned to a random class\n",
    "* Depending on the class, there will be some empty pixels in the image (0 = top-left corner, 1 = top-right, ..., 4 = none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "fake_train_class = (np.random.random((N,))*5).astype('int')\n",
    "fake_train_target = np.zeros((N,5))\n",
    "fake_train_images = np.random.random((N,10,10,1))\n",
    "\n",
    "for i in range(N):\n",
    "    c = fake_train_class[i]\n",
    "    fake_train_target[i,c] = 1\n",
    "    if( c == 0 ):\n",
    "        fake_train_images[i,:5,:5,0] = 0\n",
    "    elif( c== 1 ):\n",
    "        fake_train_images[i,:5,5:,0] = 0\n",
    "    elif( c== 2 ):\n",
    "        fake_train_images[i,5:,:5,0] = 0\n",
    "    elif( c== 3 ):\n",
    "        fake_train_images[i,5:,5:,0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use this training set to train our network. We will use a batch size of 10 (meaning that the network is trained on 10 images at a time), and go through all images once (= \"trained for 1 epoch\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "EPOCHS = 1\n",
    "\n",
    "# Reset session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    for i in range(N//BATCH_SIZE):\n",
    "        # Load batch\n",
    "        batch_X = fake_train_images[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "        batch_Y = fake_train_target[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "\n",
    "        # Perform training step\n",
    "        trainingStep.run(session=sess, feed_dict={X: batch_X, target: batch_Y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the network on the test set\n",
    "\n",
    "The network has been \"trained\", but we have no idea how well it performs ! To do that, we should generate a test set which we will use to see if the network prediction matches the target. As we are generating the examples, we can make as many as we like, so we will also use 1000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fake_test_class = (np.random.random((N,))*5).astype('int')\n",
    "fake_test_target = np.zeros((N,5))\n",
    "fake_test_images = np.random.random((N,10,10,1))\n",
    "\n",
    "for i in range(N):\n",
    "    c = fake_test_class[i]\n",
    "    fake_test_target[i,c] = 1\n",
    "    if( c == 0 ):\n",
    "        fake_test_images[i,:5,:5,0] = 0\n",
    "    elif( c== 1 ):\n",
    "        fake_test_images[i,:5,5:,0] = 0\n",
    "    elif( c== 2 ):\n",
    "        fake_test_images[i,5:,:5,0] = 0\n",
    "    elif( c== 3 ):\n",
    "        fake_test_images[i,5:,5:,0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the prediction of the network for the entire set (if the network was bigger, we would probably have to do the prediction in batches...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predicted = softmax.eval(session=sess, feed_dict={X: fake_test_images})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"predicted class\" will be the index of the highest value of the size 5 vector after the softmax operation. We can get it using the argmax operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5)\n",
      "(1000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(test_predicted.shape)\n",
    "test_predicted_class = test_predicted.argmax(axis=1)\n",
    "print(test_predicted_class.shape)\n",
    "print(fake_test_class.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the global accuracy of the test, we can just compare the predicted class vector with the target class vector :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 99.90%\n"
     ]
    }
   ],
   "source": [
    "accuracy = (test_predicted_class==fake_test_class).sum()/len(test_predicted_class)\n",
    "print(\"Accuracy : %.2f%%\"%(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very good, but the problem was extremely easy. However, you now have everything you need to create your network and apply it to the OCR problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the OCR dataset\n",
    "\n",
    "Using as a template the code from the previous exercise, provided below for simplicity, create a network which will work on the OCR problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Building the network\n",
    "X = tf.placeholder(tf.float32, [None, 10, 10, 1])\n",
    "conv = tf.layers.conv2d(X, 16, 3, activation=tf.nn.leaky_relu, kernel_initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "flat = tf.layers.flatten(conv)\n",
    "dense = tf.layers.dense(flat, 5, activation=tf.nn.leaky_relu)\n",
    "softmax = tf.nn.softmax(dense)\n",
    "\n",
    "# Preparing the training operation\n",
    "target = tf.placeholder(tf.float32, [None, 5]) # Target must have the same shape as the output\n",
    "cost = tf.nn.softmax_cross_entropy_with_logits(labels=target, logits=dense) \n",
    "cost_avg = tf.reduce_mean(cost)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "trainingStep = optimizer.minimize(cost_avg)\n",
    "\n",
    "# Train the network\n",
    "BATCH_SIZE = 10\n",
    "N = 1000\n",
    "EPOCHS = 1\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    for i in range(N//BATCH_SIZE):\n",
    "        # Load batch\n",
    "        batch_X = fake_train_images[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "        batch_Y = fake_train_target[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "\n",
    "        # Perform training step\n",
    "        trainingStep.run(session=sess, feed_dict={X: batch_X, target: batch_Y})\n",
    "\n",
    "# Test the network\n",
    "test_predicted = softmax.eval(session=sess, feed_dict={X: fake_test_images})\n",
    "test_predicted_class = test_predicted.argmax(axis=1)\n",
    "\n",
    "accuracy = (test_predicted_class==fake_test_class).sum()/len(test_predicted_class)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorBoard for visualisation and monitoring\n",
    "\n",
    "TensorBoard is a tool provided alongside TensorFlow to make it easier to inspect a network and follow the learning process. During the graph creation process, it's possible to add \"[summaries](https://www.tensorflow.org/guide/summaries_and_tensorboard)\", which can track the evolution of the loss, of weights, or any other measure you may be interested in. The summaries will also include a representation of the graph, which is useful to visualize (and debug) the network. \n",
    "\n",
    "Using the example below, which adds a summary to the fake test network, add a summary for the cost of your OCR network.\n",
    "\n",
    "To start tensorboard, open the *Anaconda Prompt* and go to the directory where this file is located, then execute the command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=./summary\n",
    "```\n",
    "\n",
    "You'll then be able to access TensorBoard from your browser.\n",
    "\n",
    "**Where would you use the \"validation set\" in the OCR data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Building the network\n",
    "X = tf.placeholder(tf.float32, [None, 10, 10, 1])\n",
    "conv = tf.layers.conv2d(X, 16, 3, activation=tf.nn.leaky_relu, kernel_initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "flat = tf.layers.flatten(conv)\n",
    "dense = tf.layers.dense(flat, 5, activation=tf.nn.leaky_relu)\n",
    "softmax = tf.nn.softmax(dense)\n",
    "\n",
    "# Preparing the training operation\n",
    "target = tf.placeholder(tf.float32, [None, 5]) # Target must have the same shape as the output\n",
    "cost = tf.nn.softmax_cross_entropy_with_logits(labels=target, logits=dense) \n",
    "cost_avg = tf.reduce_mean(cost)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "trainingStep = optimizer.minimize(cost_avg)\n",
    "\n",
    "# Train the network\n",
    "BATCH_SIZE = 10\n",
    "N = 1000\n",
    "EPOCHS = 1\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Create Summary for the cost\n",
    "tf.summary.scalar('cost_avg', cost_avg)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Create \"summary\" directory if it doesn't exist\n",
    "if( not os.path.isdir('summary') ):\n",
    "    os.mkdir('summary')\n",
    "\n",
    "train_writer = tf.summary.FileWriter('summary', sess.graph)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    for i in range(N//BATCH_SIZE):\n",
    "        # Load batch\n",
    "        batch_X = fake_train_images[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "        batch_Y = fake_train_target[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "\n",
    "        # Perform training step\n",
    "        trainingStep.run(session=sess, feed_dict={X: batch_X, target: batch_Y})\n",
    "\n",
    "        # Summary writing (note: in a real application, this could for instance be done only once per epoch)\n",
    "        summ = sess.run(merged, feed_dict={X: batch_X, target: batch_Y})\n",
    "        train_writer.add_summary(summ, i)\n",
    "\n",
    "# Test the network\n",
    "test_predicted = softmax.eval(session=sess, feed_dict={X: fake_test_images})\n",
    "test_predicted_class = test_predicted.argmax(axis=1)\n",
    "\n",
    "accuracy = (test_predicted_class==fake_test_class).sum()/len(test_predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and  loading networks\n",
    "\n",
    "For a network to be useful, we need to be able to save it, and to reload it so that it can be trained once and reused as necessary. The following code shows how to save the network, load it, and access the placeholders & output necessary for inference, once again using our fake example. You should modify this code to do the same with the OCR network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Building the network\n",
    "X = tf.placeholder(tf.float32, [None, 10, 10, 1])\n",
    "conv = tf.layers.conv2d(X, 16, 3, activation=tf.nn.leaky_relu, kernel_initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "flat = tf.layers.flatten(conv)\n",
    "dense = tf.layers.dense(flat, 5, activation=tf.nn.leaky_relu)\n",
    "softmax = tf.nn.softmax(dense)\n",
    "\n",
    "# Preparing the training operation\n",
    "target = tf.placeholder(tf.float32, [None, 5]) # Target must have the same shape as the output\n",
    "cost = tf.nn.softmax_cross_entropy_with_logits(labels=target, logits=dense) \n",
    "cost_avg = tf.reduce_mean(cost)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "trainingStep = optimizer.minimize(cost_avg)\n",
    "\n",
    "# Train the network\n",
    "BATCH_SIZE = 10\n",
    "N = 1000\n",
    "EPOCHS = 1\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Create Summary for the cost\n",
    "tf.summary.scalar('cost_avg', cost_avg)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Create \"summary\" directory if it doesn't exist\n",
    "if( not os.path.isdir('summary') ):\n",
    "    os.mkdir('summary')\n",
    "\n",
    "train_writer = tf.summary.FileWriter('summary', sess.graph)\n",
    "\n",
    "# Create \"Saver\"\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    for i in range(N//BATCH_SIZE):\n",
    "        # Load batch\n",
    "        batch_X = fake_train_images[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "        batch_Y = fake_train_target[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "\n",
    "        # Perform training step\n",
    "        trainingStep.run(session=sess, feed_dict={X: batch_X, target: batch_Y})\n",
    "\n",
    "        # Summary writing:\n",
    "        summ = sess.run(merged, feed_dict={X: batch_X, target: batch_Y})\n",
    "        train_writer.add_summary(summ, i)\n",
    "\n",
    "# Save trained network\n",
    "saver.save(sess, \"./fake_network.ckpt\") # .ckpt = tensorflow \"checkpoint\" files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load specific operations or tensors in the graph once it has been restored. To get the name of the operation we need, we can check the graph in TensorBoard. Alternatively, we can define the names when building the graph. We could for instance have written: `X = tf.placeholder(tf.float32, [None, 10, 10, 1], name='X')`, in which case we would access the operation with `X = tf.get_default_graph().get_operation_by_name('X')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained network\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./fake_network.ckpt.meta\") # Restore graph\n",
    "saver.restore(sess, \"./fake_network.ckpt\") # Restore weights\n",
    "\n",
    "# Load placeholder:\n",
    "X = tf.get_default_graph().get_operation_by_name('Placeholder').outputs[0]\n",
    "# Load network output\n",
    "output = tf.get_default_graph().get_operation_by_name('Softmax').outputs[0]\n",
    "\n",
    "# Test the network\n",
    "test_predicted = output.eval(session=sess, feed_dict={X: fake_test_images})\n",
    "test_predicted_class = test_predicted.argmax(axis=1)\n",
    "\n",
    "accuracy = (test_predicted_class==fake_test_class).sum()/len(test_predicted_class)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
